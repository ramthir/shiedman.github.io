---
title: Chapter 4. Emergence
book: 
  title: on human nature
  author: Edward O. Wilson
category: human_nature
permalink: /books/on_human_nature/04_Emergence.html
date: 2014-04-11 13:58:00 +0800
layout: book_page 
---


If biology is destiny, as Freud once told us, what becomes of free will? It is tempting to think that deep within the brain lives a soul, a free agent that takes account of the body’s experience but travels around the cranium on its own accord, reflecting, planning, and pulling the levers of the neuromotor machinery. The great paradox of determinism and free will, which has held the attention of the wisest of philosophers and psychologists for generations, can be phrased in more biological terms as follows: if our genes are inherited and our environment is a train of physical events set in motion before we were bom, how can there be a truly independent agent within the brain? The agent itself is created by the interaction of the genes and the environment. It would appear that our freedom is only a self-delusion.

In fact, this may be so. It is a defensible philosophical position that at least some events above the atomic level are predictable. To the extent that the future of objects can be foretold by an intelligence which itself has a material basis, they are determined—but only within the conceptual world of the observing intelligence. And insofar as they can make decisions of their own accord—whether or not they are determined-—they possess free will. Consider the flip of a
coin and the extent of the coin's freedom. On first thought nothing could seem less subject to determinism; coin flipping is the classic textbook example of a random process. But suppose that for some reason we decided to bring all the resources of modem science to bear on a single toss. The coin’s physical properties are measured to the nearest picogram and micron, the muscle physiology and exact contours of the flipper’s thumb are analyzed, the air currents of the room charted, the microtopography and resiliency of the floor surface mapped. At the moment of release, all of this information, plus the instantaneously recorded force and angle of the flip, are fed into a computer. Before the coin has spun through more than a few revolutions, the computer reports the expected full trajectory of the coin and its final resting position at heads or tails. The method is not perfect, and tiny errors in the initial conditions of the flip can be blown up during computation into an error concerning the outcome. Nevertheless, a series of computer-aided predictions will probably be more accurate than a series of guesses. To a limited extent, we can know the destiny of the coin.

An interesting exercise, one can reply, but not entirely relevant, because the coin has no mind. This deficiency can be remedied stepwise, by first selecting a circumstance of intermediate complexity. Let the object propelled into the air be an insect, say a honeybee. The bee has a memory. It can think in a very limited way. During its very short life—it will die of old age at fifty days—it has learned the time of day, the location of its hive, the odor of its nestmates, and the location and quality of up to five flower fields. It will respond vigorously and erratically to the flick of the scientist's hand that knocks it loose. The bee appears to be a free agent to the uninformed human observer, but again if we were to concentrate all we know about the physical properties of thimble-sized objects, the nervous system of insects, the behavioral peculiarities of honeybees, and the personal history of this particular bee, and if the most advanced computational
techniques were again brought to bear, we might predict the flight path of the bee with an accuracy that exceeds pure chance. To the circle of human observers watching the computer read-out, the future of the bee is determined to some extent. But in her own “mind” the bee, who is isolated permanently from such human knowledge, will always have free will.

When human beings ponder their own central nervous systems, they appear at first to be in the same position as the honeybee. Even though human behavior is enormously more complicated and variable than that of insects, theoretically it can be specified. Genetic constraints and the restricted number of environments in which human beings can live limit the array of possible outcomes substantially. But only techniques beyond our present imagining could hope to achieve even the short-term prediction of the detailed behavior of an individual human being, and such an accomplishment might be beyond the capacity of any conceivable intelligence. There are hundreds or thousands of variables to consider, and minute degrees of imprecision in any one of them might easily be magnified to alter the action of part or all of the mind. Furthermore, an analog of the Heisenberg uncertainty principle in subatomic physics is at work here on a grander scale: the more deeply the observer probes the behavior, the more the behavior is altered by the act of probing and the more its very meaning depends on the kinds of measurements chosen. The will and destiny of the watcher is linked to that of the person watched. Only the most sophisticated imaginable monitoring devices, capable of recording vast numbers of internal nervous processes simultaneously and from a distance, could reduce the interaction to an acceptably low level. Thus because of mathematical indeterminancy and the uncertainty principle, it may be a law of nature that no nervous system is capable of acquiring enough knowledge to significantly predict the future of any other intelligent system in detail. Nor can intelligent minds gain enough self-knowledge
to know their own future, capture fate, and in this sense eliminate free will.

An equally basic difficulty in making a forecast of an activity as complicated as the human mind lies in the transformations through which raw data reach the depths of the brain. Vision, for example, begins its journey when the radiant energy of light triggers electrical activity in the approximately one hundred million primary light receptor cells that comprise the retina. Each cell records the level of brightness {or color) that touches it in each instant of time; the image transmitted through the lens is thus picked up as a pattern of electrical signals in the manner of a television camera. Behind the retina a million or so ganglion cells receive the signals and process them by a form of abstraction. Each cell receives information from a circular cluster of primary receptors in the retina. When a light-dark contrast of sufficient intensity divides the retinal cluster, the ganglion cell is activated. This information is then passed on to a region of the cerebral cortex low in the back of the head, where special cortical nerve cells reinterpret it. Each cortical cell is activated by a group of subordinate ganglion cells. It responds with electrical activity if the pattern in which the ganglion cells are discharged reflects a straight line edge of one or the other of three particular orientations: horizontal, vertical, or oblique. Other cortical cells, carrying the abstraction still further, respond either to the ends of straight lines or to corners.

The mind might well receive all of its information, originating from both outside and inside the body, through such coding and abstracting processes. Consciousness consists of immense numbers of simultaneous and coordinated, symbolic representations by the participating neurons of the brain’s neocortex. Yet to classify consciousness as the action of organic machinery is in no way to underestimate its power. In Sir Charles Sherrington’s splendid metaphor, the brain is an “enchanted loom where millions of flashing shuttles
weave a dissolving pattern.” Since the mind recreates reality from the abstractions of sense impressions, it can equally well simulate reality by recall and fantasy. The brain invents stories and runs imagined and remembered events back and forth through time: destroying enemies, embracing lovers, carving tools from blocks of steel, travelling easily into the realms of myth and perfection.

The self is the leading actor in this neural drama. The emotional centers of the lower brain are programmed to pull the puppeteer's strings more carefully whenever the self steps onto the stage. But granted that our deepest feelings are about ourselves, can this preoccupation account for the innermost self—the soul—in mechanistic terms? The cardinal mystery of neurobioiogy is not self-love or dreams of immortality but intentionality. What is the prime mover, the weaver who guides the flashing shuttles? Too simple a neurological approach can lead to an image of the brain as a Russian doll: in the same way that we open one figure after another to reveal a smaller figure until nothing remains, our research resolves one system of neuron circuits after another into smaller subcircuits until only isolated cells remain. At the opposite extreme too complex a neurological model can lead back to a vitalistic metaphysics, in which properties are postulated that cannot be translated into neurons, circuits, or any other physical units.

The compromise solution might lie in recognizing what cognitive psychologists call schemata or plans. A schema is a configuration within the brain, either inborn or learned, against which the input of the nerve cells is compared. The matching of the real and expected patterns can have one or the other of several effects. The schema can contribute to a person's mental “set,” the screening out of certain details in favor of others, so that the conscious mind perceives a certain part of the environment more vividly than others and is likely to favor one kind of decision over another. It can fill in details that are missing from the actual sensory input and create a
pattern in the mind that is not entirely present in reality. In this way the gestalt of objects—the impression they give of being a square, a face, a tree, or whatever—is aided by the taxonomic powers of the schemata. The frames of reference serve to coordinate movement of the entire body by creating an awareness and automatic control of its moveable parts. The coupling of sensory input and these frames is dramatically illustrated when a limb has been immobilized by injury and is put back into use. A psychologist, Oliver Sacks, has described his own sensations when trying to take a first step after a long recuperation from a leg injury:



> I was suddenly precipitated into a sort of perceptual delirium, an incontinent bursting-forth of representations and images unlike anything I had ever experienced before. Suddenly my leg and the ground before me seemed immensely far away, then under my nose, then bizarrely tilted or twisted one way or another. These wild perceptions (or perceptual hypotheses) succeeded one another at the rate of several per second, and were generated in an involuntary and incalculable way. By degrees they came less erratic and wild, until finally, after perhaps five minutes and a thousand such flashes, a plausible image of the leg was achieved. With this the leg suddenly felt mine and real again, and I was forthwith able to walk.



Most significantly of all, schemata within the brain could serve as the physical basis of will. An organism can be guided in its actions by a feedback loop: a sequence of messages from the sense organs to the brain schemata back to the sense organs and on around again until the schemata “satisfy” themselves that the correct action has been completed. The mind could be a republic of such schemata, programmed to compete among themselves for control of the decision centers, individually waxing or waning in power in response to the relative urgency of the physiological needs of the body being
signaled to the conscious mind through the brain stem and midbrain. Will might be the outcome of the competition, requiring the action of neither a “little man” nor any other external agent. There is no proof that the mind works in just this way. For the moment suffice it to note that the basic mechanisms do exist; feedback loops, for example, control most of our automatic behavior. It is entirely possible that the will—the soul, if you wish—emerged through the evolution of physiological mechanisms. But, clearly, such mechanisms are far more complex than anything else on earth.

So, for the moment, the paradox of determinism and free will appears not only resolvable in theory, it might even be reduced in status to an empirical problem in physics and biology. We note that even if the basis of mind is truly mechanistic, it is very unlikely that any intelligence could exist with the power to predict the precise actions of an individual human being, as we might to a limited degree chart the path of a coin or the flight of a honeybee. The mind is too complicated a structure, and human social relations affect its decisions in too intricate and variable a manner, for the detailed histones of individual human beings to be predicted in advance by the individuals affected or by other human beings. You and I are consequently free and responsible persons in this fundamental sense.

And yet our behavior is partially determined in a second and weaker sense. If the categories of behavior are made broad enough, events can be predicted with confidence. The coin will spin and not settle on its edge, the bee will fly around the room in an upright position, and the human being will speak and conduct a wide range of social activities characteristic of the human species. Moreover, the statistical properties of populations of individuals can be specified. In the case of spinning coins, there is no need for computers and other paraphernalia to make statistical projections exact; the binomial distribution and arc-sine laws governing their behavior
can be easily written on the back of an envelope, and these mathematical formulas are rich with useful information. At another level, entomologists have produced detailed characterizations of the averaged flight patterns of honeybees to flowers. They know in advance the statistical properties of the waggle dance the bees will perform to convey the location of the flowers to nestmates. They have measured the timing and precise distribution of errors made by bees acting on that information.

To a lesser and still unknown degree the statistical behavior of human societies might be predicted, given a sufficient knowledge of human nature, the histories of the societies, and their physical environment.

Genetic determination narrows the avenue along which further cultural evolution will occur. There is no way at present to guess how far that evolution will proceed. But its past course can be more deeply interpreted and perhaps, with luck and skill, its approximate future direction can be charted. The psychology of individuals will form a key part of this analysis. Despite the imposing holistic traditions of Durkheim in sociology and Radcliffe-Brown in anthropology, cultures are not superorganisms that evolve by their own dynamics. Rather, cultural change is the statistical product of the separate behavioral responses of large numbers of human beings who cope as best they can with social existence.

When societies are viewed strictly as populations, the relationship between culture and heredity can be defined more precisely. Human social evolution proceeds along a dual track of inheritance: cultural and biological. Cultural evolution is Lamarckian and very fast, whereas biological evolution is Darwinian and usually very slow.

Lamarckian evolution would proceed by the inheritance of acquired characteristics, the transmission to offspring of traits acquired during the lifetime of the parent. When the French biologist Jean
Baptiste de Lamarck proposed the idea in 1809, he believed that biological evolution occurred in just such a manner. He suggested, for example, that when giraffes stretch their necks to feed on taller trees, their offspring acquire longer necks even without such an effort; and when storks stretch their legs to keep their bellies dry, their offspring inherit longer legs in the same direct way. Lamarckism has been entirely discounted as the basis of biological evolution, but of course it is precisely what happens in the case of cultural evolution.

The great competing theory of evolution, that entire populations are modified by natural selection, was first put in convincing form by Charles Darwin, in 1859. Individuals within populations vary in their genetic composition and thus in their ability to survive and reproduce. Those that are most successful pass more hereditary material to the next generation, and as a result the population as a whole progressively changes to resemble the successful types. Individual giraffes, by the theory of natural selection, differ from one another in the hereditary capacity to grow long necks. Those that do develop the longest necks feed more and leave the higher proportion of offspring; as a consequence the average neck length of the giraffe population increases over many generations. If, in addition, genetic mutations occurring from time to time affect neck length, the process of evolution can continue indefinitely.

Darwinism has been established as the prevailing mode of biological evolution in all kinds of organisms, including man. Because it is also far slower than Lamarckian evolution, biological evolution is always quickly outrun by cultural change. Yet the divergence cannot become too great, because ultimately the social environment created by cultural evolution will be tracked by biological natural selection. Individuals whose behavior has become suicidal or destructive to their families will leave fewer genes than those genetically less prone to such behavior. Societies that decline because of a
genetic propensity of its members to generate competitively weaker cultures will be replaced by those more appropriately endowed. I do not for a moment ascribe the relative performances of modem societies to genetic differences, but the point must be made: there is a limit, perhaps closer to the practices of contemporary societies than we have had the wit to grasp, beyond which biological evolution will begin to pull cultural evolution back to itself.

And more: individual human beings can be expected to resist too great a divergence between the two evolutionary tracks. Somewhere in the mind, as Lionel Trilling said in Beyond Culture, “there is a hard, irreducible, stubborn core of biological urgency, and biological necessity, and biological reason, that culture cannot reach and that reserves the right, which sooner or later it will exercise, to judge the culture and resist and revise it.”

Such biological refractoriness is illustrated by the failure of slavery as a human institution. Orlando Patterson, a sociologist at Harvard University, has made a systematic study of the history of slave societies around the world. He has found that true, formalized slavery passes repeatedly through approximately the same life cycle, at the end of which the peculiar circumstances stemming from its origin together with the stubborn qualities of human nature lead to its destruction.

Large-scale slavery begins when the traditional mode of production is dislocated, usually due to warfare, imperial expansion, and changes in basic crops, which in turn induces the rural free poor to migrate into the cities and newly opened colonial settlements. At the imperial center, land and capital fall increasingly under the monopoly of the rich, while citizen labor grows scarcer. The territorial expansion of the state, by making the enslavement of other peoples profitable, temporarily solves the economic problem. Were human beings then molded by the new culture, were they to behave like the red *Polyergus* ants for which slavery is an automatic response,
slave societies might become permanent. But the qualities that we recognize as most distinctively mammalian—and human—make such a transition impossible. The citizen working class becomes further divorced from the means of production because of their aversion to the low status associated with common labor. The slaves, meanwhile, attempt to maintain family and ethnic relationships and to piece together the shards of their old culture. Where the effort succeeds, many of them rise in status and alter their position from its original, purely servile form. Where self-assertion fails because it is suppressed, reproduction declines and large numbers of new slaves must be imported in each generation. The rapid turnover has a disintegrating effect on the culture of slaves and masters alike. Absenteeism rises as the slave owners attempt to spend more of their time in the centers of their own culture. Overseers come increasingly into control. Inefficiency, brutality, revolt, and sabotage increase, and the system spirals slowly downward.

Slave-supported societies, from ancient Greece and Rome to medieval Iraq and eighteenth century Jamaica, have had many other flaws, some of which might have been fatal. But the institution of slavery alone has been enough to ordain the spectacular sweep of their life cycle. “Their ascent to maturity is rapid,” Patterson writes, “their period of glory short, and their descent to oblivion ostentatious and mightily drawn out.”

The fact that slaves under great stress insist on behaving like human beings instead of slave ants, gibbons, mandrills, or any other species, is one of the reasons I believe that the trajectory of history can be plotted ahead, at least roughly. Biological constraints exist that define zones of improbable or forbidden entry. In suggesting the possibility of a certain amount of revealed destiny (a theme that will be elaborated in the final chapter), I am well aware that it is within human capacity to legislate any hypothetical course of history as opposed to another. But even if the power of self-determination
 is turned full on, the energy and materials crises solved, old ideologies defeated, and hence all societal options laid open, there are still only a few directions we will want to take. Others may be tried, but they will lead to social and economic perturbations, a decline in the quality of life, resistance, and retreat.

If it is true that history is guided to a more than negligible extent by the biological evolution that preceded it, valuable clues to its course can be found by studying the contemporary societies whose culture and economic practices most closely approximate those that prevailed during prehistory. These are the hunter-gatherers: the Australian aboriginals, Kalahari San, African pygmies, Andaman Negritos, Eskimos, and other peoples who depend entirely on the capture of animals and harvesting of free-growing plant material. Over one hundred such cultures still survive. Few contain over ten thousand members, and almost all are in danger of assimilation into surrounding cultures or outright extinction. Anthropologists, being fully aware of the great theoretical significance of these primitive cultures, are now pitted in a race against time to record them before they disappear.

Hunter-gatherers share many traits that are directly adaptive to their rugged way of life. They form bands of a hundred or less that roam over large home ranges and often divide or rejoin each other in the search for food. A group comprising twenty-five individuals typically occupies between one thousand and three thousand square kilometers, an area comparable to the home range of a wolf pack of the same size but a hundred times greater than what a troop of exclusively vegetarian gorillas would occupy. Parts of the ranges are sometimes defended as territories, especially those containing rich and reliable sources of food. Intertribal aggression, escalating in some cultures to limited warfare, is common enough to be regarded as a general characteristic of hunter-gatherer social behavior.

The band is, in reality, an extended family. Marriage is arranged
within and between bands by negotiation and ritual, and the complex kinship networks that result are objects of special classifications and strictly enforced rules. The men of the band, while leaning toward mildly polygamous arrangements, make substantial investments of time in rearing their offspring. They are also protective of their investments. Murder, which is as common per capita as in most American cities, is most often committed in response to adultery and during other disputes over women.

The young pass through a long period of cultural indoctrination during which the focus of their activities shifts gradually from the mother to age and peer groups. Their games promote physical skill but not strategy, and simulate in relatively unorganized and rudimentary form the adult roles the children will later adopt.

A strong sexual division of labor prevails in every facet of life. Men are dominant over women only in the sense of controlling certain tribal functions. They preside at councils, decide the forms of rituals, and control exchanges with neighboring groups. Otherwise, the ambience is informal and egalitarian by comparison with the majority of economically more complex societies. Men hunt and women gather. Some overlap of these roles is common, but the overlap becomes less when game is large and pursued over long distances. Hunting usually has an important but not overwhelming role in the economy. In his survey of sixty-eight hunter-gatherer societies, the anthropologist Richard B. Lee has found that on average only about one-third of the diet consists of fresh meat. Even so, this food contains the richest, most desired source of proteins and fats, and it usually confers the most prestige to its owners.

Among the many carnivores patrolling the natural environment, primitive men are unusual in capturing prey larger than themselves. Although many of the animals they pursue are small—lying within the combined size range of mice, birds, and lizards—no great creature is immune. Walruses, giraffes, kudu, and elephants fall to the
snares and hand-carved weapons of the hunters. The only other mammalian carnivores that take outsized prey are lions, hyenas, wolves, and African wild dogs. Each of these species has an exceptionally advanced social life, prominently featuring the pursuit of prey in coordinated packs. The two traits, large prey size and social hunting, are unquestionably linked. Lions, which are the only social members of the cat family, double their catch when hunting in prides. In addition they are able to subdue the largest and most difficult prey, including giraffes and adult male buffalos, which are almost invulnerable to single predators. Primitive men are ecological analogs of lions, wolves, and hyenas. Alone among the primates, with the marginal exception of the chimpanzees, they have adopted pack hunting in the pursuit of big game. And they resemble four-footed carnivores more than other primates by virtue of habitually slaughtering surplus prey, storing food, feeding solid food to their young, dividing labor, practicing cannibalism, and interacting aggressively with competing species. Bones and stone tools dug from ancient campsites in Africa, Europe, and Asia indicate that this way of life persisted for a million years or longer and was abandoned in most societies only during the last few thousands of years. Thus the selection pressures of hunter-gatherer existence have persisted for over 99 percent of human genetic evolution.

This apparent correlation between ecology and behavior brings us to the prevailing theory of the origin of human social behavior. It consists of a series of interlocking reconstructions that have been fashioned from bits of fossil evidence, extrapolations back through time from hunter-gatherer societies, and comparisons with other living primate species. The core of the theory is what. I referred to in my earlier book *Sociobiology* as the *autocatalysis model*. Autocatalysis is a term that originated in chemistry; it means any process that increases in speed according to the amount of the products it has created. The longer the process runs, the greater its speed. By
this conception the earliest men or man-apes started to walk erect when they came to spend most or all of their time on the ground. Their hands were freed, the manufacture and handling of artifacts were made easier, and intelligence grew as the tool-using habit improved. With mental capacity and the tendency to use artifacts increasing through mutual reinforcement, the entire materials-based culture expanded. Now the species moved onto the dual track of evolution: genetic evolution by natural selection enlarged the capacity for culture, and culture enhanced the genetic fitness of those who made maximum use of it. Cooperation during hunting was perfected and provided a new impetus for the evolution of intelligence, which in turn permitted still more sophistication in tool using, and so on through repeated cycles of causation. The sharing of game and other food contributed to the honing of social skills. In modem hunter-gatherer bands, it is an occasion for constant palavering and maneuvering. As Lee said of the !Kung San,

The buzz of conversation is a constant background to the camp’s activities: there is an endless flow of talk about gathering, hunting, the weather, food distribution, gift giving, and scandal. No !Kung is ever at a loss for words, and often two or three people will hold forth at once in a single conversation, giving the listeners a choice of channels to tune in on. A good proportion of this talk in even the happiest of camps verges on argument. People argue about improper food division, about breaches of etiquette, and about failure to reciprocate hospitality and gift giving ... Almost all the arguments are *ad bominem*. The most frequent accusations heard are of pride, arrogance, laziness, and selfishness.

The natural selection generated by such exchanges might have been enhanced by the more sophisticated social behavior required by the female's nearly continuous sexual accessibility. Because a high
level of cooperation exists within the band, sexual selection would be linked with hunting prowess, leadership, skill at tool making, and other visible attributes that contribute to the strength of the family and the male band. At the same time aggressiveness would have to be restrained and the phylogenetically ancient forms of overt primate dominance replaced by complex social skills. Young males would find it profitable to fit into the group by controlling their sexuality and aggression and awaiting their turn at leadership. The dominant male in these early hominid societies was consequently most likely to possess a mosaic of qualities that reflect the necessities of compromise. Robin Fox has suggested the following portrait: “Controlled, cunning, cooperative, attractive to the ladies, good with the children, relaxed, tough, eloquent, skillful, knowledgeable and proficient in self-defense and hunting.” Because there would have been a continuously reciprocating relationship between die more sophisticated social traits and breeding success, social evolution could continue indefinitely without additional selective pressures from the environment.

At some point, possibly during the transition from the more primitive Australopithecus man-apes to the earliest true men, the autocatalysis carried the evolving populations to a new threshold of competence, at which time the hominids were able to exploit the sivatheres, elephants, and other large herbivorous animals teeming around them on the African plains. Quite possibly the process began when the hominids learned to drive big cats, hyenas, and other carnivores away from their kills. In time the hominids became the primary hunters and were forced to protect their prey from other predators and scavengers.

Child care would have been improved by close social bonding between individual males, who left the domicile to hunt larger game, and individual females, who kept the children and conducted most of the foraging for vegetable food. In a sense, love was added to
sex. Many of the peculiar details of human sexual behavior and domestic life flow easily from this basic division of labor. But such details are not essential to the autocatalysis model. They are appended to the evolutionary story only because they are displayed by virtually all hunter-gatherer societies.

Autocatalytic reactions never expand to infinity, and biological processes themselves normally change through time to slow growth and eventually bring it to a halt. But almost miraculously, this has not yet happened in human evolution. The increase in brain size and refinement of stone artifacts point to an unbroken advance in mental ability over the last two to three million years. During this crucial period the brain evolved in either one great surge or a series of alternating surges and plateaus. No organ in the history of life has grown faster. When true men diverged from the ancestral man-apes, the brain added one cubic inch—about a tablespoonful— every hundred thousand years. The rate was maintained until about one quarter of a million years ago, when, at about the time of the appearance of the modem species *Homo sapiens*, it tapered off. Physical growth was then supplanted by an increasingly prominent cultural evolution. With the appearance of the Mousterian tool culture of the Neanderthal man some seventy-five thousand years ago, cultural change gathered momentum, giving rise in Europe to the Upper Paleolithic culture of Cro-Magnon man about forty thousand years before the present. Starting about ten thousand years ago agriculture was invented and spread, populations increased enormously in density, and the primitive hunter-gatherer bands gave way locally to the relentless growth of tribes, chiefdoms, and states. Finally, after A.D. 1400 European-based civilization shifted gears again, and the growth of knowledge and technology accelerated to world-altering levels.

There is no reason to believe that during this final sprint to the space age there has been a cessation in the evolution of either mental
capacity or the predilection toward special social behaviors. The theory of population genetics and experiments on other organisms show that substantial changes can occur in the span of less than 100 generations, which for man reaches back only to the time of the Roman Empire. Two thousand generations, roughly the time since typical Homo sapiens invaded Europe, is enough time to create new species and to mold their anatomy and behavior in major ways. Although we do not know how much mental evolution has actually occurred, it would be premature to assume that modern civilizations have been built entirely on genetic capital accumulated during the long haul of the Ice Age.

That capital is nevertheless very large. It seems safe to assume that the greater part of the changes that transpired in the interval from the hunter-gatherer life of forty thousand years ago to the first glimmerings of civilization in the Sumerian city states, and virtually all of the changes from Sumer to Europe, were created by cultural rather than genetic evolution. The question of interest, then, is the extent to which the hereditary qualities of hunter-gatherer existence have influenced the course of subsequent cultural evolution.

I believe that the influence has been substantial. In evidence is the fact that the emergence of civilization has everywhere followed a definable sequence. As societies grew in size from the tiny hunter-gatherer bands, the complexity of their organization increased by the addition of features that appeared in a fairly consistent order. As band changed to tribe, true male leaders appeared and gained dominance, alliances between neighboring groups were strengthened and formalized, and rituals marking the changes of season became general. With still denser populations came the attributes of generic chiefdom: the formal distinction of rank according to membership in families, the hereditary consolidation of leadership, a sharper division of labor, and the redistribution of wealth under the control of the ruling elite. As chiefdoms gave rise in turn to cities and states,
these basic qualities were intensified. The hereditary status of the elite was sanctified by religious beliefs. Craft specialization formed the basis for stratifying the remainder of society into classes. Religion and law were codified, armies assembled, and bureaucracies expanded. Irrigation systems and agriculture were perfected, and as a consequence populations grew still denser. At the apogee of the state’s evolution, architecture was monumental, and the ruling classes were exalted as a pseudospecies. The sacred rites of statehood became the central focus of religion.

The similarities between the early civilizations of Egypt, Mesopotamia, India, China, Mexico, and Central and South America in these major features are remarkably close. They cannot be explained away as the products of chance or cultural cross-fertilization. It is true that the archives of ethnography and history are filled with striking and unquestionably important variations in the details of culture, but it is the parallelism in the major features of organization that demands our closest attention in the consideration of die theory of the dual track of human social evolution.

In my opinion the key to the emergence of civilization is *hypertrophy*, the extreme growth of pre-existing structures. Like the teeth of the baby elephant that lengthen into tusks, and the cranial bones of the male elk that sprout into astonishing great antlers, the basic social responses of the hunter-gatherers have metamorphosed from relatively modest environmental adaptations into unexpectedly elaborate, even monstrous forms in more advanced societies. Yet the directions this change can take and its final products are constrained by the genetically influenced behavioral predispositions that constituted the earlier, simpler adaptations of preliterate human beings.

Hypertrophy can sometimes be witnessed at the beginning. One example in its early stages is the subordination of women in elementary cultures. The !Kung San of the Kalahari Desert do not impose sex roles on their children. Adults treat little girls in apparently the

[>> CHART: Type of society](society_chart.html)

same manner as little boys, which is to say with considerable indulgence and permissiveness. Yet, as the anthropologist Patricia Draper found during a special study of child development, small average differences still appear. From the beginning the girls stay closer to home and join groups of working adults less frequently. During play, boys are more likely to imitate the men, and girls are more likely to imitate the women. As the children grow up, these differences lead through imperceptible steps to a still stronger difference in adult sex roles. Women gather mongongo nuts and other plant food and fetch water, usually within a mile of camp, while men range farther in search of game. But !Kung social life is relaxed and egalitarian, and tasks are often shared. Men sometimes gather mongongo nuts or build huts (women’s work), with or without their families, and women occasionally catch small game. Both sexual roles are varied and esteemed by all. According to Draper, !Kung women maintain personal control over the food they gather, and in demeanor they are generally “vivacious and self-confident.”

In a few localities bands have settled into villages to take up farming. The work is heavier, and for the first time in known !Kung history it has come to be shared to a significant extent by the younger children. The sexual roles are noticeably hardened from early childhood onward. Girls stay even closer to the home than previously in order to care for smaller children and perform household chores. Boys tend herds of domestic animals and protect the gardens from monkeys and goats. By maturity the sexes have diverged far from one another in both way of life and status. The women are more frilly domestic, working almost continuously at a multiplicity of tasks in which they are supervised. The men continue to wander freely, taking responsibility for their own time and activities.

So only a single lifetime is needed to generate the familiar pattern of sexual domination in a culture. When societies grow still larger and more complex, women tend to be reduced in influence outside
the home, and to be more constrained by custom, ritual, and formal law. As hypertrophy proceeds further, they can be turned literally into chattel, to be sold and traded, fought over, and ruled under a double morality. History has seen a few striking local reversals, but the great majority of societies have evolved toward sexual domination as though sliding along a ratchet.

Most and perhaps all of the other prevailing characteristics of modem societies can be identified as hypertrophic modifications of the biologically meaningful institutions of hunter-gatherer bands and early tribal states. Nationalism and racism, to take two examples, are the culturally nurtured outgrowths of simple tribalism. Where the Nyae Nyae !Kung speak of themselves as perfect and clean and other !Kung people as alien murderers who use deadly poisons, civilizations have raised self-love to the rank of high culture, exalted themselves by divine sanction and diminished others with elaborately falsified written histories.

Even the beneficiaries of the hypertrophy have found it difficult to cope with extreme cultural change, because they are sociobiologically equipped only for an earlier, simpler existence. Where the hunter-gatherer fills at most one or two informal roles out of only several available, his literate counterpart in an industrial society must choose ten or more out of thousands, and replace one set with another at different periods of his life or even at different times of the day. Furthermore, each occupation—the physician, the judge, the teacher, the waitress—is played just so, regardless of the true workings of the mind behind the persona. Significant deviations in performance are interpreted by others as a sign of mental incapacity and unreliability. Daily life is a compromised blend of posturing for the sake of role-playing and of varying degrees of self-revelation. Under these stressful conditions even the “true” self cannot be precisely defined, as Erving Goffrnan observes.

> There is a relation between persons and role. But the relationship answers to the interactive system—to the frame—in which the role is performed and the self of the performer is glimpsed. Self, then, is not an entity half-concealed behind events, but a changeable formula for managing oneself during them. Just as the current situation prescribes the official guise behind which we will conceal ourselves, so it provides where and how we will show through, the culture itself prescribing what sort of entity we must believe ourselves to be in order to have something to show through in this manner.



Little wonder that the identity crisis is a major source of modem neuroticism, and that the urban middle class aches for a return to a simpler existence.

As these various cultural superstructures have proliferated, their true meaning more often than not has become lost to the practitioners. In Cannibals and Kings, Marvin Harris has suggested a series of bizarre examples of the way that chronic meat shortages affect the shaping of religious beliefs. While die ancient hunter-gatherers were beset with daily perils and constricting fluctuations in the environments that kept their populations low in density, they could at least count on a relatively high fraction of fresh meat in their diet. Early human beings, as I have said, filled a special ecological niche: they were the carnivorous primates of the African plains. They retained this position throughout the Ice Age as they spread into Europe, Asia, and finally into Australia and the New World. When agriculture permitted the increase of population density, game was no longer abundant enough to provide a sufficient supply of fresh meat, and the rising civilizations either switched to domestic animals or went on reduced rations. But in either case carnivorism remained a basic dietary impulse, with cultural aftereffects that varied 
according to the special conditions of the environment in which the society evolved.

Ancient Mexico, like most of the forest-invested New World tropics, was deficient in the land of large game that flourished on the plains of Africa and Asia. Furthermore, the Aztecs and other peoples who built civilizations there failed to domesticate animals as significant sources of meat. As human populations grew thicker in the Valley of Mexico, the Aztec ruling class was still able to enjoy such delicacies as dogs, turkeys, ducks, deer, rabbits, and fish. But animal flesh was virtually eliminated from the diets of the commoners, who were occasionally reduced to eating clumps of spirulina algae skimmed from the surface of Lake Texcoco. The situation was partially relieved by cannibalizing the victims of human sacrifice. As many as fifteen thousand persons a year were being consumed in the Valley of Mexico when Cortez entered. The conquistadors found a hundred thousand skulls stacked in neat rows in the plaza at Xocotlan and another 136 thousand at Tenochtitlán. The priesthood said that human sacrifice was approved by the high gods, and they sanctified it with elaborate rituals performed amid statuary of the gods placed on imposing white temples erected for this purpose. But these trappings should not distract us from the fact that immediately after their hearts had been cut out, the victims were systematically butchered like animals and their parts distributed and eaten. Those favored in the feasts included the nobility, their retainers, and the soldiery, in other words the groups with the greatest political power.

India began from a stronger nutrient base than Mexico and followed a different but equally profound cultural transformation as meat grew scarce. The earlier Aryan invaders of the Gangetic Plain presided over feasts of cattle, horses, goats, buffalo, and sheep. By later Vedic and early Hindu times, during the first millenium B.C.,
the feasts came to be managed by the priestly caste of Brahmans, who erected rituals of sacrifice around the killing of animals and distributed the meat in the name of the Aryan chiefs and war lords. After 600 B.C., when populations grew denser and domestic animals became proportionately scarcer, the earing of meat was progressively restricted until it became a monopoly of the Brahmans and their sponsors. Ordinary people struggled to conserve enough livestock to meet their own desperate requirements for milk, dung used as fuel, and transport. During this period of crisis, reformist religions arose, most prominently Buddhism and Jainism, that attempted to abolish castes and hereditary priesthoods and to outlaw the killing of animals. The masses embraced the new sects, and in the end their powerful support reclassified the cow into a sacred animal.

So it appears that some of the most baffling of religious practices in history might have an ancestry passing in a straight line back to the ancient carnivorous habits of humankind. Cultural anthropologists like to stress that the evolution of religion proceeds down multiple, branching pathways. But these pathways are not infinite in number; they may not even be very numerous. It is even possible that with a more secure knowledge of human nature and ecology, the pathways can be enumerated and the directions of religious evolution in individual cultures explained with a high level of confidence.

I interpret contemporary human social behavior to comprise hypertrophic outgrowths of the simpler features of human nature joined together into an irregular mosaic. Some of the outgrowths, such as the details of child care and of kin classification, represent only slight alterations that have not yet concealed their Pleistocene origins. Others, such as religion and class structure, are such gross transmutations that only the combined resources of anthropology and history can hope to trace their cultural phylogeny back to 
rudiments in the hunter-gatherers’ repertory. But even these might in time be subject to a statistical characterization consistent with biology.

The most extreme and significant hypertrophic segment is the gathering and sharing of knowledge. Science and technology expand at an accelerating rate in ways that alter our existence year by year, To judge realistically the magnitude of that growth, note that it is already within our reach to build computers with the memory capacity of a human brain. Such an instrument is admittedly not very practical: it would occupy most of the space of the Empire State Building and draw down an amount of energy equal to half the output of the Grand Coulee Dam. In the 1980s, however, when new “bubble memory” elements already in the experimental stage are added, the computer might be shrunk to fill a suite of offices on one floor of the same building. Meanwhile, advances in storage and retrieval are matched by increases in the rate of flow of information. During the past twenty-five years transoceanic telephone calls and amateur radio transmission have increased manyfold, television has become global, the number of books and journals has grown exponentially, and universal literacy has become the goal of most nations. The fraction of Americans working in occupations concerned primarily with information has increased from 20 to nearly 50 percent of the work force.

Pure knowledge is the ultimate emancipator. It equalizes people and sovereign states, erodes the archaic barriers of superstition and promises to lift the trajectory of cultural evolution. But I do not believe it can change the ground rules of human behavior or alter the main course of history’s predictable trajectory. Self-knowledge will reveal the elements of biological human nature from which modern social life proliferated in all its strange forms. It will help to distinguish safe from dangerous future courses of action with greater precision. We can hope to decide more judiciously which of the
elements of human nature to cultivate and which to subvert, which to take open pleasure with and which to handle with care. We will not, however, eliminate the hard biological substructure until such time, many years from now, when our descendents may learn to change the genes themselves. With that basic proposition having been stated, I now invite you to reconsider four of the elemental categories of behavior, aggression, sex, altruism, and religion, on the basis of sociobiological theory.

